{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be87c92",
   "metadata": {},
   "source": [
    "# Bird Classification using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788d4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91cae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/media/grzetan/445C33B25C339E1C/datasets/birds'\n",
    "\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, root_dir, random_state=42, transform=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.root = root_dir\n",
    "        self.classes = os.listdir(self.root)\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.paths = []\n",
    "        self.labels = []\n",
    "        for cls in self.classes:\n",
    "            for p in os.listdir(os.path.join(self.root, cls)):\n",
    "                self.paths.append(os.path.join(self.root, cls, p))\n",
    "                self.labels.append(self.classes.index(cls))\n",
    "        self.paths = np.array(self.paths)\n",
    "        self.labels = np.array(self.labels)\n",
    "        idx = np.random.permutation(len(self.paths))\n",
    "        self.paths = self.paths[idx]\n",
    "        self.labels = self.labels[idx]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = np.asarray(Image.open(self.paths[idx]))\n",
    "        \n",
    "        sample = (img, label)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20081dbc",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5916baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        img, label = sample\n",
    "        img = img / 255\n",
    "        img = img.transpose((2,0,1))\n",
    "        return (torch.from_numpy(img).to(torch.float), torch.tensor(label).to(torch.int64))\n",
    "    \n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        img, label = sample\n",
    "        img = (img - self.mean[:,None,None]) / self.std[:,None,None]\n",
    "        return img, label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb83e5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/grzetan/445C33B25C339E1C/datasets/birds/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6466/1634173360.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.4451\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4262\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3959\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2411\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2403\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2466\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_set = BirdDataset(os.path.join(DATASET_DIR, 'train'), \n\u001b[0m\u001b[1;32m      4\u001b[0m                       transform=transforms.Compose([ToTensor(), normalize]))\n\u001b[1;32m      5\u001b[0m test_set = BirdDataset(os.path.join(DATASET_DIR, 'test'), \n",
      "\u001b[0;32m/tmp/ipykernel_6466/4012666286.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, random_state, transform)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/grzetan/445C33B25C339E1C/datasets/birds/train'"
     ]
    }
   ],
   "source": [
    "normalize = Normalize(mean=[0.4451, 0.4262, 0.3959], std=[0.2411, 0.2403, 0.2466])\n",
    "\n",
    "train_set = BirdDataset(os.path.join(DATASET_DIR, 'train'), \n",
    "                      transform=transforms.Compose([ToTensor(), normalize]))\n",
    "test_set = BirdDataset(os.path.join(DATASET_DIR, 'test'), \n",
    "                      transform=transforms.Compose([ToTensor(), normalize]))\n",
    "val_set = BirdDataset(os.path.join(DATASET_DIR, 'valid'), \n",
    "                      transform=transforms.Compose([ToTensor(), normalize]))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64)\n",
    "test_loader = DataLoader(test_set, batch_size=64)\n",
    "val_loader = DataLoader(val_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet18(pretrained=True)\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc78cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().cpu())\n",
    "    print(len(layers), len(ave_grads))\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, epochs=25):\n",
    "    start = time.time()\n",
    "    \n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch}/{epochs-1}')\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            \n",
    "            dataloader = train_loader if phase == 'train' else val_loader\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(dataloader):\n",
    "                print(f'\\r{i / len(dataloader)}%', end='')\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        plot_grad_flow(model.named_parameters())\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_acc += torch.sum(preds == labels)\n",
    "        \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            # Stats\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_acc = running_acc / len(dataloader)\n",
    "            \n",
    "            print(f'{phase} - loss: {epoch_loss}, acc - {epoch_acc}')\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    t = time.time() - start\n",
    "    print(f'Training completed in {t}, best accuracy: {best_acc}')\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "in_features = resnet.fc.in_features\n",
    "\n",
    "# Freeze all the layers except last one \n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet.fc = torch.nn.Linear(in_features, train_set.n_classes)\n",
    "resnet.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916d5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = train_model(resnet, criterion, optimizer, scheduler, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd17755",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/grzetan/PYTHON/Exercises-From-Hands-On-Machine-Learning/14-ComputerVision/with_freezed_layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ca493",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = train_model(model, criterion, optimizer, scheduler, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/grzetan/PYTHON/Exercises-From-Hands-On-Machine-Learning/14-ComputerVision/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corr, test_loss = 0, 0.0\n",
    "\n",
    "for input, label in test_loader:\n",
    "    input = input.to(device)\n",
    "    label = label.to(device)\n",
    "    output = model(input)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    loss = criterion(output, label)\n",
    "    test_corr += torch.sum(preds == label)\n",
    "    test_loss += loss.item() * input.size(0)\n",
    "    \n",
    "test_acc = test_corr / len(test_loader)\n",
    "test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc}, test loss - {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
