{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696200a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "if not 'shakespeare.txt' in os.listdir('./'):\n",
    "    r = requests.get('https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "    open('shakespeare.txt', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6052b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F': 0, 'i': 1, 'r': 2, 's': 3, 't': 4, ' ': 5, 'C': 6, 'z': 7, 'e': 8, 'n': 9, ':': 10, '\\n': 11, 'B': 12, 'f': 13, 'o': 14, 'w': 15, 'p': 16, 'c': 17, 'd': 18, 'a': 19, 'y': 20, 'u': 21, 'h': 22, ',': 23, 'm': 24, 'k': 25, '.': 26, 'A': 27, 'l': 28, 'S': 29, 'Y': 30, 'v': 31, '?': 32, 'R': 33, 'M': 34, 'W': 35, \"'\": 36, 'L': 37, 'I': 38, 'N': 39, 'g': 40, ';': 41, 'b': 42, '!': 43, 'O': 44, 'j': 45, 'V': 46, '-': 47, 'T': 48, 'H': 49, 'E': 50, 'U': 51, 'D': 52, 'P': 53, 'q': 54, 'x': 55, 'J': 56, 'G': 57, 'K': 58, 'Q': 59, '&': 60, 'Z': 61, 'X': 62, '3': 63, '$': 64}\n",
      "{0: 'F', 1: 'i', 2: 'r', 3: 's', 4: 't', 5: ' ', 6: 'C', 7: 'z', 8: 'e', 9: 'n', 10: ':', 11: '\\n', 12: 'B', 13: 'f', 14: 'o', 15: 'w', 16: 'p', 17: 'c', 18: 'd', 19: 'a', 20: 'y', 21: 'u', 22: 'h', 23: ',', 24: 'm', 25: 'k', 26: '.', 27: 'A', 28: 'l', 29: 'S', 30: 'Y', 31: 'v', 32: '?', 33: 'R', 34: 'M', 35: 'W', 36: \"'\", 37: 'L', 38: 'I', 39: 'N', 40: 'g', 41: ';', 42: 'b', 43: '!', 44: 'O', 45: 'j', 46: 'V', 47: '-', 48: 'T', 49: 'H', 50: 'E', 51: 'U', 52: 'D', 53: 'P', 54: 'q', 55: 'x', 56: 'J', 57: 'G', 58: 'K', 59: 'Q', 60: '&', 61: 'Z', 62: 'X', 63: '3', 64: '$'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# For every character create distinct ID and encode the text\n",
    "n_chars = 0\n",
    "\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    char2ids = {}\n",
    "    idx2char = {}\n",
    "    encoded = []\n",
    "    for l in f.readlines():\n",
    "        for char in l:\n",
    "            n_chars += 1\n",
    "            if char not in char2ids.keys():\n",
    "                char2ids[char] = len(char2ids)\n",
    "                idx2char[len(char2ids)-1] = char\n",
    "            encoded.append(char2ids[char])    \n",
    "    encoded = np.array(encoded, dtype=np.int8)\n",
    "    \n",
    "print(char2ids)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165a8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, encoded, n_unique_chars, window_size=101):\n",
    "        self.n_unique_chars = n_unique_chars\n",
    "        self.encoded = torch.tensor(encoded, dtype=torch.float)\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.window_size - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window = self.encoded[idx:idx+self.window_size]\n",
    "        return window[:-1].long(), window[1:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d649500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(n_chars * 0.9) # Take 90% of text as training data\n",
    "print(train_size)\n",
    "train_dataset = ShakespeareDataset(encoded[:train_size], len(char2ids))\n",
    "train_dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf9cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device('cuda' if not torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76d34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, X, hidden):\n",
    "        X = self.embed(X)\n",
    "        X, hidden = self.gru(X.unsqueeze(1), hidden)\n",
    "        X = self.out(X.reshape(X.shape[0], -1))\n",
    "        return X, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X, hidden, cell):\n",
    "        print(X.shape)\n",
    "        out = self.embed(X)\n",
    "        print(out.shape)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        print(out.shape)\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        print(out.shape)\n",
    "        input()\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c6983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+=+++++++++\n",
      "Tyjoy$pXx'D$myCgja'Rq?UMPLV\n",
      "&;li,L!fjWJw3cJVM,V\n",
      "iI!pxr\n",
      "cnqx?BLwD;bDq3qxlMjT uOKCNKBFa;?Ls,AEIGx\n",
      "!zuT&xYxlJiGixE'TIVJNwkj:Gqcaofk'COBl:vJg;WYl&Mr3W?BU:cUfbFv'qqkpveLE-qGBRKcYiE3begVUgPw!sOKzulUSZF'dn\n",
      "ELe\n",
      "+++++++++++++++++\n",
      " 2.603546447753906+=+++++++++\n",
      "Ty beare wie;\n",
      "I INdd me your weard veou ave theat the inr am?\n",
      "\n",
      "Tuste the I Burish.\n",
      "\n",
      "UU:\n",
      "I dan; ounc ararl the od mind doughu\n",
      "Wiml ord woh a dout af, are be;\n",
      "R Ey oour shae qwavot go hit los mllorey mang\n",
      "+++++++++++++++++\n",
      " 2.1876789855957033+=+++++++++\n",
      "Ty leke ir to her wey.f\n",
      "Yithe,, conte.\n",
      "\n",
      "OMISCE:\n",
      "Cull were butre with spepenteresllou whe kny thur with nout prits wor or, mrang oraveed\n",
      "The for sleePosd wuor! my;\n",
      "Thaver and her, Sralk; moun a apcher'.\n",
      "\n",
      "+++++++++++++++++\n",
      " 1.9981024169921875+=+++++++++\n",
      "Ty firse though in paeld\n",
      "Shas me as bow man hin the on sners, shis the and knop, shave not, gordsanse.\n",
      "\n",
      "VINALA:\n",
      "I degtiren\n",
      "Pos in and me this this soume ad nese bing as sand the shid an thour you dich'd\n",
      "+++++++++++++++++\n",
      " 2.0877114868164064+=+++++++++\n",
      "Ty sere stance,\n",
      "Ther seord thee soect.\n",
      "\n",
      "Whe, hearthever. \n",
      "Ageyars,\n",
      "And he; by cemane therr ffor nobut the e poresafl\n",
      "A thee here to your a to in jet tut ferance a sprancing for say noy but.\n",
      "Seruse ent i\n",
      "+++++++++++++++++\n",
      " 2.054151916503906+=+++++++++\n",
      "Ty may.\n",
      "\n",
      "KENTONHINA INGE:\n",
      "So inge can the then aply sive low as am the ir terther's am, consme.\n",
      "\n",
      "DUDOKE\n",
      "I to the it tive he deshis thele ay,\n",
      "I to he this thy bool!\n",
      "\n",
      "Voth cut ming thou desepon wor beesn'\n",
      "+++++++++++++++++\n",
      " 2.079641876220703+=+++++++++\n",
      "Ty seter the hear,\n",
      "This with beaves fellud me will it by spentole\n",
      "queren's ap his sallanbughten of and came an'thive. I'll greey and ENWhat ming compmen you,, my this you hearter this though you you dea\n",
      "+++++++++++++++++\n",
      " 1.9788494873046876+=+++++++++\n",
      "TyWer;\n",
      "Sure not you in a proth'd of dratch what I'll lord fawand sir, me bund in I soul; mer'd promaen worll hen in fairs, but you, a lifble and\n",
      "Lord, your will do will be enare steer, an'd you good- An\n",
      "+++++++++++++++++\n",
      " 1.9139166259765625+=+++++++++\n",
      "Ty a buch bleat of have is the hall\n",
      "Prom a which gracte the sin, an mremight to clat,\n",
      "Shall Edward I hath coulsed for camk of is the.\n",
      "\n",
      "ROMENIO:\n",
      "Thoids, theyet besh seat' well were,\n",
      "The senter Corues\n",
      "Man\n",
      "+++++++++++++++++\n",
      " 1.717909698486328+=+++++++++\n",
      "Ty:\n",
      "Whost thus grase to but his not how your may!\n",
      "\n",
      "KINGARD II:\n",
      "Saits, my princh wiveen corntmell worst his loyy\n",
      "Of suy.\n",
      "\n",
      "KING LICAR:\n",
      "Thou all this hourse of the to death:\n",
      "Wadred light beath.\n",
      "So call sir\n",
      "+++++++++++++++++\n",
      " 1.8317442321777344+=+++++++++\n",
      "Ty to me pert do come your post unway may resir: the craity\n",
      "Than pleate bly with turm to be for the the goot.\n",
      "\n",
      "MANRENTENS:\n",
      "Which come hall shill reate.\n",
      "\n",
      "AUTINUS: Cableed diced\n",
      "And, this proof him sting \n",
      "+++++++++++++++++\n",
      " 1.5216128540039062+=+++++++++\n",
      "Ty a change of in that clith.\n",
      "\n",
      "KING ELIZAR$Hy\n",
      "here I wold rois: whates tnain, thy great this starde gante honen's streat.\n",
      "\n",
      "KING RICHARTIUS:\n",
      "Upore of you let's knower\n",
      "And of ewrive vain, thous,\n",
      "I lood do\n",
      "+++++++++++++++++\n",
      " 1.726068572998047+=+++++++++\n",
      "Ty frients, me out not to mant yours are the canst\n",
      "But witbert a preaces at instate, drofing.\n",
      "\n",
      "MENENIUS:\n",
      "Now call us Herard's freets,\n",
      "And did fair and they there meracting this hinds,\n",
      "O's his have to be\n",
      "+++++++++++++++++\n",
      " 1.84301025390625+=+++++++++\n",
      "Ty the beart if sucht of huse to the though\n",
      "And thou in out, on to the know'd or here sprighter:\n",
      "To so't their it the power defore to some be to the king bleamal triuth:\n",
      "By sutters farminess the shight \n",
      "+++++++++++++++++\n",
      " 1.9469039916992188+=+++++++++\n",
      "Ty offer you how who a seeg's condenty:\n",
      "Which a troy the contly drought\n",
      "And and Agant,\n",
      "And the creath, all, dan\n",
      "it recommen, blefold is this look humulguin his nord?\n",
      "\n",
      "ANGELO:\n",
      "With this sen, I hame, what\n",
      "+++++++++++++++++\n",
      " 1.6344606018066405+=+++++++++\n",
      "Ty brown!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Yirt, you,--\n",
      "\n",
      "GRUCEL:\n",
      "Here, which the frize, do my him onour.\n",
      "How my greant unpery tain of hath good you like my blaid,\n",
      "Nought, and I gim my dearby, my have brange\n",
      "Thanlow nev\n",
      "+++++++++++++++++\n",
      " 1.5971795654296874+=+++++++++\n",
      "Ty rugre what's the the worlow let the stande.\n",
      "\n",
      "QUEENTENSIA:\n",
      "Prremen the werelenasted lody comity dange,\n",
      "A would but the earren know, what mest not comes.\n",
      "\n",
      "ESTANUE:\n",
      "I battent, love did musting this hand\n",
      "+++++++++++++++++\n",
      " 1.6152711486816407+=+++++++++\n",
      "Ty loveth art Jurmly beful affightle this,\n",
      "A good be him fore courn to lass\n",
      "Or day and of then coop agown both of to care offence too but then ever pringlant,\n",
      "And cromet come a courth dost poy say not, \n",
      "+++++++++++++++++\n",
      " 1.8288902282714843+=+++++++++\n",
      "Ty,\n",
      "So pritter curse a wery borthen or some in the dillow to so me: fatchered's nother cround;\n",
      "Are in terpul that not I wing it these his daught earrow,\n",
      "To am right his in Engrate ure to cortseriented p\n",
      "+++++++++++++++++\n",
      " 1.4473606872558593+=+++++++++\n",
      "Ty reod;\n",
      "And siren well than have and both for your before who's you so meather didst.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Than I confold her worn the larnes was plany curs; let nameor it with bes of so\n",
      "porson: the men ha\n",
      "+++++++++++++++++\n",
      " 1.7776383972167968+=+++++++++\n",
      "Ty, I that judged is,\n",
      "As decnay a manbrohfer from thel?\n",
      "\n",
      "LICTwBRUCKIO:\n",
      "With thy the fares, can more Mry\n",
      "plise consent onj thee, be mend here would flound\n",
      "Had a mend dursends af that's the hope the have\n",
      "\n",
      "+++++++++++++++++\n",
      " 1.8093609619140625+=+++++++++\n",
      "Ty serving shought in enemes inderivel:\n",
      "But is his let it here and in them, which\n",
      "For this by intermable came to yet a them.\n",
      "\n",
      "PLARENCE:\n",
      "My lord; thou nought, which sound soul.\n",
      "\n",
      "VARENB:\n",
      "If is isealf I hu\n",
      "+++++++++++++++++\n",
      " 1.688585205078125+=+++++++++\n",
      "Ty: yet our pray,\n",
      "That keoling I'll of this bust of as my kinsting though it. And have it stot mornich,\n",
      "And so my tell, iting and not, as our\n",
      "As ming I'll bethis bittentlase you elffice this set my gear\n",
      "+++++++++++++++++\n",
      " 1.6022862243652343"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10036/1736383562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10036/668056520.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(char2ids), 256, 2, len(char2ids)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "check_every = 100\n",
    "\n",
    "def generate(model, init_str=\"Ty\", prediction_len=200, temperature=0.85):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    init_inp = torch.tensor([char2ids[char] for char in init_str]).long()\n",
    "    pred = init_str\n",
    "\n",
    "    for p in range(len(init_str) - 1):\n",
    "        _, hidden = model(init_inp[p].view(1).to(device), hidden)\n",
    "\n",
    "    last_char = init_inp[-1]\n",
    "\n",
    "    for p in range(prediction_len):\n",
    "        output, hidden = model(last_char.view(1).to(device), hidden)\n",
    "        output_distance = output.data.view(-1).div(temperature).exp()\n",
    "        top_char = torch.multinomial(output_distance, 1)[0]\n",
    "        predicted_char = idx2char[int(top_char)]\n",
    "        pred += predicted_char\n",
    "        last_char = torch.tensor(char2ids[predicted_char])\n",
    "\n",
    "    return pred\n",
    "\n",
    "for epoch in range(10):\n",
    "    for j, (window, target) in enumerate(loader):\n",
    "        model.zero_grad()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        window = window.to(device)\n",
    "        target = target.to(device)\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(window.shape[1]): \n",
    "            out, hidden = model(window[:,i], hidden)  \n",
    "            loss += criterion(out, target[:,i])\n",
    "\n",
    "        if j%check_every == 0:\n",
    "            print('+=+++++++++')\n",
    "            print(generate(model))\n",
    "            print(\"+++++++++++++++++\")\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'\\r {loss.item() / window.shape[1]}', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd47a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
