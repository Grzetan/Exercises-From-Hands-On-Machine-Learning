{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fbbf26",
   "metadata": {},
   "source": [
    "# Spam detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8e67f",
   "metadata": {},
   "source": [
    "## Fetch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5df7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "MAIN_URL = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "TAR_FILES = ['20030228_easy_ham.tar.bz2', '20030228_easy_ham_2.tar.bz2', '20030228_hard_ham.tar.bz2',\n",
    "            '20030228_spam.tar.bz2', '20050311_spam_2.tar.bz2']\n",
    "DATASET_PATH = './dataset'\n",
    "\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    shutil.rmtree(DATASET_PATH)\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = os.path.join(DATASET_PATH, 'dataset.tar.bz2')\n",
    "\n",
    "def fetch_dataset(url, path=DATASET_PATH):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(OUTPUT_FILE, 'wb') as f:\n",
    "            f.write(r.raw.read())\n",
    "        file = tarfile.open(OUTPUT_FILE, 'r:bz2')\n",
    "        file.extractall(path=DATASET_PATH)\n",
    "        file.close()\n",
    "      \n",
    "for url in TAR_FILES:\n",
    "    fetch_dataset(os.path.join(MAIN_URL, url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb379bea",
   "metadata": {},
   "source": [
    "## Move files to two directories: spam and ham (not spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d22452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPAM_DIR = \"spam\"\n",
    "HAM_DIR = \"ham\"\n",
    "\n",
    "if os.path.exists(SPAM_DIR):\n",
    "    shutil.rmtree(SPAM_DIR)\n",
    "\n",
    "if os.path.exists(HAM_DIR):\n",
    "    shutil.rmtree(HAM_DIR)\n",
    "\n",
    "os.makedirs(SPAM_DIR)\n",
    "os.makedirs(HAM_DIR)\n",
    "\n",
    "spam_count = 0\n",
    "not_spam_count = 0\n",
    "\n",
    "for d in os.listdir(DATASET_PATH): \n",
    "    if '_ham' in d:\n",
    "        for f in os.listdir(os.path.join(DATASET_PATH, d)):\n",
    "            if 'cmds' in f:\n",
    "                continue\n",
    "\n",
    "            shutil.move(os.path.join(DATASET_PATH, *[d,f]), os.path.join(HAM_DIR, f'ham_{not_spam_count}'))\n",
    "            not_spam_count += 1\n",
    "    elif 'spam' in d:\n",
    "        for f in os.listdir(os.path.join(DATASET_PATH, d)):\n",
    "            if 'cmds' in f:\n",
    "                continue\n",
    "            shutil.move(os.path.join(DATASET_PATH, *[d,f]), os.path.join(SPAM_DIR, f'spam_{spam_count}'))\n",
    "            spam_count += 1\n",
    "\n",
    "#Delete dataset dir\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    shutil.rmtree(DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad0681",
   "metadata": {},
   "source": [
    "## Make train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b03812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209,)\n",
      "(4837,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create 2d array (one row for every email) \n",
    "def create_emails_list():\n",
    "    emails = []\n",
    "    # 0 - spam\n",
    "    # 1 - ham\n",
    "    for i in range(len(os.listdir(SPAM_DIR))):\n",
    "        emails.append([f'spam_{i}', 0])\n",
    "    for i in range(len(os.listdir(HAM_DIR))):\n",
    "        emails.append([f'ham_{i}',1])\n",
    "    emails = np.array(emails)\n",
    "    np.random.shuffle(emails)\n",
    "    return emails\n",
    "\n",
    "def split_train_test(emails, test_radio=0.2):\n",
    "    thresh = int(test_radio * len(emails))\n",
    "    return emails[:thresh,0], emails[:thresh,1], emails[thresh:,0], emails[thresh:,1] \n",
    "    \n",
    "emails = create_emails_list()\n",
    "test_x, test_y, train_x, train_y = split_train_test(emails)\n",
    "print(test_x.shape)\n",
    "print(train_x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188fd1a",
   "metadata": {},
   "source": [
    "## Classes for transforming each email to sparse word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d25482be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailWordCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keep_header=False):\n",
    "        self.keep_header = keep_header\n",
    "    \n",
    "    def get_words(self, x):\n",
    "        dirr = HAM_DIR if 'ham' in x else SPAM_DIR\n",
    "        f = open(os.path.join(dirr, x), encoding='utf-8', errors='ignore')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        # Strip off email header\n",
    "        if not self.keep_header:\n",
    "            for i, l in enumerate(lines):\n",
    "                if l == '\\n':\n",
    "                    lines = lines[i+1:]\n",
    "                    break\n",
    "        # Join lines and remove punctuation\n",
    "        text = \" \".join(lines).lower()\n",
    "        words = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        return words\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        words_in_emails = []\n",
    "        # Collect words\n",
    "        for i, email in enumerate(x):\n",
    "            wordsCounter = {}\n",
    "            for w in self.get_words(email):\n",
    "                if w.startswith('http') or w.startswith('www'):\n",
    "                    w = 'URL'\n",
    "                elif any(char.isdigit() for char in w):\n",
    "                    w = 'NUMBER'\n",
    "\n",
    "                if w in wordsCounter:\n",
    "                    wordsCounter[w] += 1\n",
    "                else:\n",
    "                    wordsCounter[w] = 1\n",
    "            words_in_emails.append(wordsCounter)\n",
    "            \n",
    "        return np.array(words_in_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0b332",
   "metadata": {},
   "source": [
    "## Indexes to sklearn's sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41ccb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "class WordsToSparse(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dict_size=1000):\n",
    "        self.dict_size = dict_size\n",
    "        self.dict = []\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        final_words = Counter({})\n",
    "        for email in x:\n",
    "            final_words += email\n",
    "        self.dict = [val[0] for val in final_words.most_common()][:self.dict_size]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, email in enumerate(x):\n",
    "            for word, count in email.items():\n",
    "                rows.append(row)\n",
    "                data.append(count)\n",
    "                col = 0 if word not in self.dict else self.dict.index(word) + 1\n",
    "                cols.append(col)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(x), len(self.dict) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55617a0",
   "metadata": {},
   "source": [
    "## Create pipeline to transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d93140db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4837x1001 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 592547 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('emailToWords', EmailWordCounter(keep_header=True)),\n",
    "    ('WordsToSparse', WordsToSparse())\n",
    "])\n",
    "\n",
    "prepared = pipeline.fit_transform(train_x)\n",
    "prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d085aae",
   "metadata": {},
   "source": [
    "## Test RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "806b05aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805669352146564"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "score = cross_val_score(forest, prepared, train_y, cv=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb815b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "forest.fit(prepared, train_y)\n",
    "pred = forest.predict(prepared)\n",
    "precision = precision_score(pred, train_y, pos_label='1')\n",
    "recall = recall_score(pred, train_y, pos_label='1')\n",
    "\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc0ec2",
   "metadata": {},
   "source": [
    "## Test LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cd6c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9940173496859108\n",
      "0.9913484486873508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grzetan/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(prepared, train_y)\n",
    "\n",
    "pred = model.predict(prepared)\n",
    "precision = precision_score(pred, train_y, pos_label='1')\n",
    "recall = recall_score(pred, train_y, pos_label='1')\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974216a",
   "metadata": {},
   "source": [
    "## Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57d8020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9838909541511772\n",
      "0.9887920298879203\n"
     ]
    }
   ],
   "source": [
    "prepared_test = pipeline.transform(test_x)\n",
    "\n",
    "pred = forest.predict(prepared_test)\n",
    "\n",
    "precision = precision_score(pred, test_y, pos_label='1')\n",
    "recall = recall_score(pred, test_y, pos_label='1')\n",
    "\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a92f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3810jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
